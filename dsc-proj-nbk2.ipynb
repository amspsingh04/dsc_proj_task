{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install Keras-Preprocessing\n!pip install gensim\nimport gensim.downloader as api \nword_embeddings = api.load(\"glove-twitter-100\")  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-22T15:11:06.864299Z","iopub.execute_input":"2024-06-22T15:11:06.865002Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: Keras-Preprocessing in /opt/conda/lib/python3.10/site-packages (1.1.2)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from Keras-Preprocessing) (1.26.4)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from Keras-Preprocessing) (1.16.0)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.4)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.4.0)\n[==------------------------------------------------] 5.8% 22.4/387.1MB downloaded","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import Adam\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nimport os\nimport gensim\nfrom gensim.models import Word2Vec \n\ndata = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None)\ndata.columns = ['target','ids','date','flag','user','text']\nenglish_stopwords = stopwords.words('english')\nstemmer = SnowballStemmer('english')\nregex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\ndef preprocess(text, stem=False):\n  text = re.sub(regex, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in english_stopwords:\n      tokens.append(stemmer.stem(token))\n  return \" \".join(tokens)\ndata.text = data.text.apply(lambda x: preprocess(x))\ntrain, test = train_test_split(data, test_size=0.1, random_state=44)\nprint('Train dataset shape: {}'.format(train.shape))\nprint('Test dataset shape: {}'.format(test.shape))\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train.text)  \nvocab_size = len(tokenizer.word_index) + 1 \nmax_length = 50\nsequences_train = tokenizer.texts_to_sequences(train.text) \nsequences_test = tokenizer.texts_to_sequences(test.text) \nX_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')\nX_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\ny_train = train.target.values\ny_test = test.target.values\nembeddings_dictionary = dict()\nembedding_dim = 100\n\nglove_file = open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\nglove_file.close()\nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[index] = embedding_vector\nembedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)\nnum_epochs = 10\nbatch_size = 1000\nmodel = Sequential([\n        embedding_layer,\n        tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Bidirectional(LSTM(128)),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])\ntf.keras.utils.plot_model(model, show_shapes=True)\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size = batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)\ny_pred = model.predict(X_test)\ny_pred = np.where(y_pred>0.5, 1, 0)\nprint(classification_report(y_test, y_pred))\nmodel.save('tsanlp.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T14:32:39.156181Z","iopub.execute_input":"2024-06-22T14:32:39.156670Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nTrain dataset shape: (1440000, 6)\nTest dataset shape: (160000, 6)\n[===-----------------------------------------------] 6.8% 26.4/387.1MB downloaded","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"[=============================---------------------] 59.9% 231.8/387.1MB downloaded","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}