{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477},{"sourceId":7563141,"sourceType":"datasetVersion","datasetId":4403839}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-22T13:53:29.069178Z","iopub.status.idle":"2024-06-22T13:53:29.069649Z","shell.execute_reply.started":"2024-06-22T13:53:29.069402Z","shell.execute_reply":"2024-06-22T13:53:29.069420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import re \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport time\nimport string\n\nDATASET_COLUMNS=['target','ids','date','flag','user','text']\nDATASET_ENCODING = \"latin-1\"\ndf = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', \n                 encoding=DATASET_ENCODING, \n                 names=DATASET_COLUMNS)\ndata = df[['text','target']]\ndata_pos = data[data['target'] == 4]\ndata_net = data[data['target'] == 2]\ndata_neg = data[data['target'] == 0]\ndataset = pd.concat([data_pos, data_net, data_neg])\ndataset['text'] = dataset['text'].str.lower()\nnltk.download('stopwords')\nSTOPWORDS = set(stopwords.words('english'))\ndef cleaning_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\nenglish_punctuations = string.punctuation\npunctuations_list = english_punctuations\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\ndef cleaning_repeating_char(text):\n    return re.sub(r'(.)1+', r'1', text)\ndef cleaning_URLs(data):\n    return re.sub('((www.[^s]+)|(https?://[^s]+))', ' ', data)\ndef cleaning_numbers(data):\n    return re.sub('[0-9]+', '', data)\n                  \n\ndataset['text'] = dataset['text'].apply(lambda x: cleaning_punctuations(x))\ndataset['text'] = dataset['text'].apply(lambda x: cleaning_repeating_char(x))\ndataset['text'] = dataset['text'].apply(lambda x: cleaning_URLs(x))\ndataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))                  \ndataset['text'] = dataset['text'].apply(word_tokenize)\nst = nltk.PorterStemmer()\ndef stemming_on_text(data):\n    text = [st.stem(word) for word in data]\n    return data\nlm = nltk.WordNetLemmatizer()\ndef lemmatizer_on_text(data):\n    text = [lm.lemmatize(word) for word in data]\n    return data\ndataset['text'] = dataset['text'].apply(lambda x: stemming_on_text(x))\ndataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\nX = data.text\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=26105111)\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)\n                  \ndef model_Evaluate(model):\n    y_pred = model.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    categories = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n    labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n    xticklabels = categories, yticklabels = categories)\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n\nprint(\"8\")\n\nBNBmodel = BernoulliNB()\nstart = time.time()\nBNBmodel.fit(X_train, y_train)\nend = time.time()\nprint(\"The execution time of this model is {:.2f} seconds\\n\".format(end-start))\nmodel_Evaluate(BNBmodel)\ny_pred1 = BNBmodel.predict(X_test)\n\nSVCmodel = LinearSVC()\nstart = time.time()\nSVCmodel.fit(X_train, y_train)\nend = time.time()\nprint(\"The execution time of this model is {:.2f} seconds\\n\".format(end-start))\nmodel_Evaluate(SVCmodel)\ny_pred2 = SVCmodel.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}